{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Coursework 1 Coding Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pandas as pd\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialise condition for question a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.array([1, 2, 3, 4])\n",
    "data_y = np.array([3, 2, 0, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(x, degree):\n",
    "    return np.array([x**i for i in range(degree + 1)]).T\n",
    "\n",
    "def linear_regression(X, y):\n",
    "    return np.linalg.solve(X.T @ X, X.T @ y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "x_plot = np.linspace(0, 5, 500)\n",
    "\n",
    "for degree in range(0, 4):\n",
    "    X = extract_features(data_x, degree)\n",
    "    coeffs = linear_regression(X, data_y).round(2)\n",
    "    X_plot = extract_features(x_plot, degree)\n",
    "    y_plot = X_plot @ coeffs\n",
    "    plt.plot(x_plot, y_plot, label=f'k={degree}')\n",
    "\n",
    "plt.scatter(data_x, data_y, color='red', label='Data Points')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.ylim(-5, 10)\n",
    "plt.title('Polynomial Fits for Different Degrees')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "ax = plt.gca()\n",
    "ax.spines['bottom'].set_position(('data', 0))\n",
    "ax.spines['left'].set_position(('data', 0))\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for degree in range(0, 4):\n",
    "    X = extract_features(data_x, degree)\n",
    "    coeffs = linear_regression(X, data_y).round(2)\n",
    "    print(f\"Degree {degree+1} polynomial coefficients: {coeffs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for degree in range(0, 4):\n",
    "    X = extract_features(data_x, degree)\n",
    "    coeffs = linear_regression(X, data_y)\n",
    "    coeffs = coeffs.round(2)\n",
    "    y_pred = X @ coeffs\n",
    "    print(y_pred)\n",
    "    mse = np.mean((data_y - y_pred) ** 2)\n",
    "    print(f\"MSE for k={degree}: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2a i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_sigma(x, sigma):\n",
    "    noise = np.random.normal(0, sigma, size=len(x))\n",
    "    return np.sin(2 * np.pi * x)**2 + noise\n",
    "\n",
    "# 生成数据并绘图\n",
    "np.random.seed(0)\n",
    "x_sample = np.random.uniform(0,1,30)\n",
    "y_sample = g_sigma(x_sample, sigma=0.07)\n",
    "\n",
    "x_plot = np.linspace(0, 1, 500)\n",
    "y_plot = np.sin(2 * np.pi * x_plot)**2\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_plot, y_plot, label='sin^2(2πx)')\n",
    "plt.scatter(x_sample, y_sample, color='red', label='Noisy Data Points')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Function with Noise')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2a ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [2, 5, 10, 14, 18]\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_plot = np.linspace(0, 1, 500)\n",
    "for degree in degrees:\n",
    "    X = extract_features(x_sample, degree-1)\n",
    "    coeffs = linear_regression(X, y_sample)\n",
    "    X_plot = extract_features(x_plot, degree-1)\n",
    "    y_plot = X_plot @ coeffs\n",
    "    print(coeffs)\n",
    "    plt.plot(x_plot, y_plot, label=f'k={degree}')\n",
    "\n",
    "\n",
    "plt.scatter(x_sample, y_sample, color='red', label='Data Points')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Polynomial Fits for Different Degrees')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(-1.5, 1.5)\n",
    "ax = plt.gca()\n",
    "ax.spines['bottom'].set_position(('data', 0))\n",
    "ax.spines['left'].set_position(('data', 0))\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_errors = []\n",
    "degrees = [i for i in range(1,19)]\n",
    "for degree in range(0,18):\n",
    "    X = extract_features(x_sample, degree)\n",
    "    coeffs = linear_regression(X, y_sample)\n",
    "    y_pred = X @ coeffs\n",
    "    mse = np.mean((y_sample-y_pred)**2)\n",
    "    training_errors.append(mse)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, np.log(training_errors), marker='o')\n",
    "plt.xlabel('Polynomial Dimension (k)')\n",
    "plt.ylabel('ln(MSE)')\n",
    "plt.title('Log Training Error vs Polynomial Dimension')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(training_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x_test = np.random.uniform(0,1,1000)\n",
    "y_test = g_sigma(x_test, sigma=0.07)\n",
    "\n",
    "testing_errors = []\n",
    "degrees = [i for i in range(1,19)]\n",
    "for degree in range(0,18):\n",
    "    X = extract_features(x_sample, degree)\n",
    "    coeffs = linear_regression(X, y_sample)\n",
    "    X = extract_features(x_test, degree)\n",
    "    y_pred = X @ coeffs\n",
    "    mse = np.mean((y_test-y_pred)**2)\n",
    "    testing_errors.append(mse)\n",
    "for i in testing_errors:\n",
    "    print(i)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, np.log(testing_errors), marker='o')\n",
    "plt.xlabel('Polynomial Dimension (k)')\n",
    "plt.ylabel('ln(MSE)')\n",
    "plt.title('Log Testing Error vs Polynomial Dimension')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_testing_errors = [0 for i in range(18)]\n",
    "for experiment in range(100):\n",
    "    np.random.seed(experiment)\n",
    "    sub_x_sample = np.random.uniform(0,1,30)\n",
    "    sub_y_sample = g_sigma(sub_x_sample, sigma=0.07)\n",
    "    for degree in range(0,18):\n",
    "        X = extract_features(sub_x_sample, degree)\n",
    "        coeffs = linear_regression(X, sub_y_sample)\n",
    "        X = extract_features(x_test, degree)\n",
    "        y_pred = X @ coeffs\n",
    "        mse = np.mean((y_test-y_pred)**2)\n",
    "        testing_errors.append(mse)\n",
    "    total_testing_errors  = [x + y for x, y in zip(total_testing_errors, testing_errors)]\n",
    "average_testing_errors = [x/100 for x in total_testing_errors]\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in average_testing_errors:\n",
    "    print(i)\n",
    "plt.plot([i for i in range(1,19)], np.log(average_testing_errors), marker='o')\n",
    "plt.xlabel('Polynomial Dimension (k)')\n",
    "plt.ylabel('ln(avg MSE)')\n",
    "plt.title('Log Average Testing Error vs Polynomial Dimension')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_trig(x, degree):\n",
    "    return np.column_stack([np.sin((i + 1) * np.pi * x) for i in range(degree)])\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "x_test = np.random.uniform(0, 1, 1000)\n",
    "y_test = g_sigma(x_test, sigma=0.07)\n",
    "\n",
    "\n",
    "n_experiments = 100 \n",
    "degrees = [i for i in range(1, 19)]  \n",
    "\n",
    "x_sample = np.random.uniform(0, 1, 50)\n",
    "y_sample = g_sigma(x_sample, sigma=0.07)\n",
    "\n",
    "testing_errors = []\n",
    "\n",
    "for degree in degrees:\n",
    "    X_train = extract_features_trig(x_sample, degree)\n",
    "    coeffs = linear_regression(X_train, y_sample)\n",
    "    \n",
    "    X_test = extract_features_trig(x_test, degree)\n",
    "    y_pred = X_test @ coeffs\n",
    "    mse = np.mean((y_test - y_pred) ** 2)\n",
    "    testing_errors.append(mse)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, np.log(testing_errors), marker='o')\n",
    "plt.xlabel('Basis Dimension (k)')\n",
    "plt.ylabel('ln(MSE)')\n",
    "plt.title('Log Testing Error vs Basis Dimension (Trig Basis)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "testing_errors_all = []\n",
    "\n",
    "for experiment in range(n_experiments):\n",
    "   \n",
    "    x_sample = np.random.uniform(0, 1, 50)\n",
    "    y_sample = g_sigma(x_sample, sigma=0.07)\n",
    "  \n",
    "    testing_errors = []\n",
    "    \n",
    "    for degree in degrees:\n",
    "       \n",
    "        X_train = extract_features_trig(x_sample, degree)\n",
    "        coeffs = linear_regression(X_train, y_sample)\n",
    "        \n",
    "        # 提取测试特征并计算误差\n",
    "        X_test = extract_features_trig(x_test, degree)\n",
    "        y_pred = X_test @ coeffs\n",
    "        mse = np.mean((y_test - y_pred) ** 2)\n",
    "        testing_errors.append(mse)\n",
    "    \n",
    "    testing_errors_all.append(testing_errors)\n",
    "\n",
    "average_testing_errors = np.mean(testing_errors_all, axis=0)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, np.log(average_testing_errors), marker='o')\n",
    "plt.xlabel('Basis Dimension (k)')\n",
    "plt.ylabel('ln(avg MSE)')\n",
    "plt.title('Log Average Testing Error vs Basis Dimension (Trig Basis)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_link = \"http://www.cs.ucl.ac.uk/staff/M.Herbster/boston-filter/Boston-filtered.csv\"\n",
    "\n",
    "data = pd.read_csv(csv_link)\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1]  # All columns except the last\n",
    "y = data.iloc[:, -1] \n",
    "results = {\"method\": [], \"train_mse\": [], \"test_mse\": []}\n",
    "Naive_result = defaultdict(list)\n",
    "Single_result = {f'Single Attribute{i+1}':defaultdict(list)for i in range(X.shape[1])}\n",
    "All_result = defaultdict(list)\n",
    "# Define number of runs\n",
    "num_runs = 20\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    # Split data into training and testing sets (2/3 for training, 1/3 for testing)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=None)\n",
    "\n",
    "    # (a) Predicting with the mean y-value on the training set\n",
    "    ones_train = np.ones_like(y_train).reshape(-1, 1)\n",
    "    ones_test = np.ones_like(y_test).reshape(-1, 1)\n",
    "\n",
    "    w_naive = linear_regression(ones_train, y_train)\n",
    "\n",
    "    y_train_pred_naive = ones_train@w_naive\n",
    "    y_test_pred_naive = ones_test@w_naive\n",
    "\n",
    "    train_mse_naive = np.mean((y_train - y_train_pred_naive) ** 2)\n",
    "    test_mse_naive = np.mean((y_test - y_test_pred_naive) ** 2)\n",
    "\n",
    "    Naive_result[\"train_mse\"].append(train_mse_naive)\n",
    "    Naive_result[\"test_mse\"].append(test_mse_naive)\n",
    "\n",
    "    # (b) Predicting with a single attribute and a bias term\n",
    "    for i in range(X.shape[1]):\n",
    "        X_single_train = X_train.iloc[:, i].values.reshape(-1, 1)\n",
    "        X_single_test = X_test.iloc[:, i].values.reshape(-1, 1)\n",
    "\n",
    "        X_single_train = X_single_train.reshape(-1)\n",
    "        X_single_train = extract_features(X_single_train, 1)\n",
    "        w_single = linear_regression(X_single_train, y_train)\n",
    "\n",
    "        X_single_test = X_single_test.reshape(-1)\n",
    "        X_single_test = extract_features(X_single_test, 1)\n",
    "\n",
    "        y_train_pred_singhe = X_single_train@w_single\n",
    "        y_test_pred_single = X_single_test@w_single\n",
    "\n",
    "        train_mse_single = np.mean((y_train-y_train_pred_singhe)**2)\n",
    "        test_mse_single = np.mean((y_test-y_test_pred_single)**2)\n",
    "\n",
    "        Single_result[f'Single Attribute{i+1}'][\"train_mse\"].append(train_mse_single)\n",
    "        Single_result[f'Single Attribute{i+1}'][\"test_mse\"].append(test_mse_single)\n",
    "\n",
    "    # (c) Predicting with all attributes\n",
    "    w_all = linear_regression(X_train, y_train)\n",
    "\n",
    "    y_train_pred_all = X_train@w_all\n",
    "    y_test_pred_all = X_test@w_all\n",
    "\n",
    "    train_mse_all = np.mean((y_train-y_train_pred_all)**2)\n",
    "    test_mse_all = np.mean((y_test-y_test_pred_all)**2)\n",
    "\n",
    "    All_result[\"train_mse\"].append(train_mse_all)\n",
    "    All_result[\"test_mse\"].append(test_mse_all)\n",
    "\n",
    "Naive_result[\"train_mse\"] = np.mean(Naive_result[\"train_mse\"])\n",
    "Naive_result[\"test_mse\"] = np.mean(Naive_result[\"test_mse\"])\n",
    "\n",
    "print('question a')\n",
    "print(\"Naive method:\")\n",
    "print(f\"Train MSE: {Naive_result['train_mse']}\")\n",
    "print(f\"Test MSE: {Naive_result['test_mse']}\")\n",
    "print('')\n",
    "print('question b')\n",
    "print('The naive regression in problem a simply computes the mean of the training set and uses that as the prediction for all test data points.')\n",
    "print('')\n",
    "print('question c')\n",
    "for i in range(X.shape[1]):\n",
    "    Single_result[f'Single Attribute{i+1}'][\"train_mse\"] = np.mean(Single_result[f'Single Attribute{i+1}'][\"train_mse\"])\n",
    "    Single_result[f'Single Attribute{i+1}'][\"test_mse\"] = np.mean(Single_result[f'Single Attribute{i+1}'][\"test_mse\"])\n",
    "    print(f\"Single Attribute {i+1}:\")\n",
    "    print(f\"Train MSE: {Single_result[f'Single Attribute{i+1}']['train_mse']}\")\n",
    "    print(f\"Test MSE: {Single_result[f'Single Attribute{i+1}']['test_mse']}\")\n",
    "    print('')\n",
    "print('question d')\n",
    "All_result[\"train_mse\"] = np.mean(All_result[\"train_mse\"])\n",
    "All_result[\"test_mse\"] = np.mean(All_result[\"test_mse\"])\n",
    "print(\"All Attributes:\")\n",
    "print(f\"Train MSE: {All_result['train_mse']}\")\n",
    "print(f\"Test MSE: {All_result['test_mse']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Generating the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(77)\n",
    "S = 100\n",
    "X = np.random.rand(S, 2)\n",
    "y = np.random.choice([0, 1], size = S)\n",
    "\n",
    "def h_S_v(x_S, y_S, x, v):\n",
    "    distances = np.linalg.norm(x_S - x, axis=1)\n",
    "    nearest_indices = np.argsort(distances)[:v]\n",
    "    nearest_labels = y_S[nearest_indices]\n",
    "\n",
    "    return np.bincount(nearest_labels).argmax()\n",
    "\n",
    "def plot_boundary(x_S, y_S, v, resolution=200):\n",
    "\n",
    "    # Build Grid\n",
    "    x_min, x_max = 0, 1\n",
    "    y_min, y_max = 0, 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, resolution), np.linspace(y_min, y_max, resolution))\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    predictions = np.array([h_S_v(x_S, y_S, point, v) for point in grid_points])\n",
    "    Z = predictions.reshape(xx.shape)\n",
    "\n",
    "    custom_cmap = ListedColormap(['white', 'turquoise'])\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.contourf(xx, yy, Z, alpha = 0.5, cmap = custom_cmap)\n",
    "    plt.scatter(x_S[y_S == 0][:,0], x_S[y_S == 0][:,1], c = 'green', label = \"Label 0\")\n",
    "    plt.scatter(x_S[y_S == 1][:,0], x_S[y_S == 1][:,1], c = 'blue', label = \"Label 1\")\n",
    "    plt.title(\"Figure of h_{S,v}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_boundary(X, y , v = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Estimated generalization error of k-NN as a function of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise(S, v=3, size=1000, noise_prob=0.2):\n",
    "    x_S, y_S = S\n",
    "    X = np.random.rand(size, 2)\n",
    "    y = []\n",
    "    for x in X:\n",
    "        if np.random.rand() > noise_prob:\n",
    "            y.append(h_S_v(x_S, y_S, x, v))\n",
    "        else:\n",
    "            y.append(np.random.choice([0,1]))\n",
    "    return X, np.array(y)\n",
    "\n",
    "def generalize_err(S, k_values, runs = 100, train_size = 1000, test_size = 1000):\n",
    "    mean_errors = []\n",
    "    for k in k_values:\n",
    "        errors = []\n",
    "        for _ in range(runs):\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine_Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
